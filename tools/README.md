### 背景
    目前工业界和学术界均无安全能力的测试评价标准，都是通过在若干样本集上的测试结果对应用安全工具进行评价的。样本集的设计没有标准，差异很大，无法全面客观的反映全貌，且评价结果只能得出一个总体的检出率、遗漏率和误报率数据，无法直观反映安全能力的优势和不足。
    为了能全面而客观的对安全能力进行评价，蚂蚁安全团队从2021年开始探索建立安全能力的评价体系。通过应用安全评价体系项目，希望可以衡量出应用安全产品的技术优劣，指引应用安全产品的发展方向，并可辅助用于商业化安全产品采购的技术选型，详见wiki。
    项目主要包括两部分，一部分是分语言的评价体系，另一部分是基于评价体系的Benchmark。本bundle下发布的是Java SAST引擎能力评价体系V1.0和Benchmark。如果您有任何好的想法，欢迎与我们团队联系。
### 项目结构
```
└── src
    ├── main
    │   ├── java
    │   │   └── com
    │   │       └── sast
    │   │           └── astbenchmark
    │   │               ├── cases   
      
```
### 快速开始
#### 测试流程简介
SAST引擎能力测评包括三步：</br>
（1）准备前置条件</br>
（2）测试目标产品/引擎扫描SAST靶场</br>
（3）自主分析靶场测试结果 或 自动化获取评价体系测试结果 </br>
#### 准备前置条件
（1）确认被测的SAST产品配置了以下sink点规则
```
Runtime.getRuntime().exec() -- 参数位置0
org.apache.http.impl.client.CloseableHttpClient#execute -- 参数位置0
cn.hutool.http.HttpRequest#post -- 参数位置0
java.net.URL#openConnection -- URL对象为污点被调用
java.sql.Statement#executeQuery -- 参数位置0
SinkUtil#sink (本评价体系靶场自定义) -- 参数位置0
```
#### 自主分析靶场测试结果
自主分析测试产品/引擎的输出，结合case路由对应相应的评价项，分析该款产品当前可以检出的场景、不可检出的场景以及存在误报的场景有哪些。从而分析出测试产品/引擎的能力。

#### 自动化获取评价体系测试结果
我们配套建设了一款基于测试产品的结果自动化生成测试结果的工具，可以自动将SAST产品的输出结果和评价项进行对应并输出测评报告，免去自主分析的繁琐工作。具体可见tools目录的readme

### License
This project is licensed under the Apache License 2.0
